{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C S 363D HW 6\n",
    "\n",
    "# Ensemble Methods and Skewed Data\n",
    "\n",
    "## Ana Williams and Fronrich Puno\n",
    "\n",
    "For this week's homework we are going explore two ensemble methods:\n",
    "\n",
    "  - AdaBoost, and\n",
    "  - Random Forests\n",
    "  \n",
    "Along with applying different KPIs (key performance indicators) that are more appropriate to highly skewed data sets. \n",
    "\n",
    "The dataset contains transactions made by credit cards in September 2013 by european cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a [PCA transformation](https://en.wikipedia.org/wiki/Principal_component_analysis). Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount in Euros. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "At the end of this homework, I expect you to understand how to train and use ensemble classifiers, how to characterize model performance with ROC curves, and be familiar with the difference between accuracy, true positive rate, and positive predictive value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the libraries you will use for this assignment, you may not import anything else\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# This is the credit card data provided, we'll use sklearn methods to do cross validation\n",
    "# to estimate error\n",
    "df_cc = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view data\n",
    "df_cc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Parition the data for cross validation\n",
    "\n",
    "Load the data, and split the data set into $X$ (the feature dataframe, `df_X`) and $y$ (the target series `s_y`). Define our partitions.  \n",
    "\n",
    "We know this is a _super_ skewed data set, so we worry about our target class being underrepresented in a random k-fold selection. With this in mind, we use a [stratifed k-fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html), since it will preserve our class balance in our experiements. Use $k=10$, . Instantiate an instance of the `StratifiedKFold` class, and use the generator `split` to populate the test and train dictonaries:\n",
    "   - `d_train_df_X` : key is the fold number, value is the attribute training dataframe at that fold\n",
    "   - `d_test_df_X`  : key is the fold number, value is the attribute test dataframe at that fold\n",
    "   - `d_train_s_y`  : key is the fold number, value is the target training series at that fold\n",
    "   - `d_train_s_y`  : key is the fold number, value is the target test series at that fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target is defined as class\n",
    "target_col = 'Class'\n",
    "\n",
    "df_X = df_cc.drop(columns=[target_col])\n",
    "s_y = df_cc[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_df_X = dict()\n",
    "d_test_df_X = dict()\n",
    "d_train_s_y = dict()\n",
    "d_test_s_y = dict()\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# convert to numpy arrays\n",
    "df_X_array = np.array(df_X)\n",
    "s_y_array = np.array(s_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iteration\n",
    "# there will be 10 folds\n",
    "index_fold = 0\n",
    "\n",
    "for index_train, index_test in skf.split(df_X_array,s_y_array): \n",
    "    \n",
    "    # assign df_X\n",
    "    d_train_df_X[index_fold] = pd.DataFrame(df_X_array[index_train])\n",
    "    d_test_df_X[index_fold] = pd.DataFrame(df_X_array[index_test])\n",
    "    \n",
    "    # assign s_y\n",
    "    d_train_s_y[index_fold] = pd.Series(s_y_array[index_train])\n",
    "    d_test_s_y[index_fold] = pd.Series(s_y_array[index_test])\n",
    "    \n",
    "    # sniff\n",
    "    # print(\"Fold: \" + str(index_fold))\n",
    "    # print(\"d_train_df_X: \" + str(d_train_df_X[index_fold]))\n",
    "    # print(\"d_test_df_X: \" + str(d_test_df_X[index_fold]))\n",
    "    # print(\"d_train_s_y: \" + str(d_train_s_y[index_fold]))\n",
    "    # print(\"d_test_s_y: \" + str(d_test_s_y[index_fold]))\n",
    "    \n",
    "    # increment fold\n",
    "    index_fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sniff test d_...\n",
    "# print(\"d_train_df_X: \" + str(d_train_df_X))\n",
    "# print(\"d_test_df_X: \" + str(d_test_df_X))\n",
    "# print(\"d_train_s_y: \" + str(d_train_s_y))\n",
    "# print(\"d_test_s_y: \" + str(d_test_s_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    28432\n",
      "1       49\n",
      "dtype: int64\n",
      "0    28432\n",
      "1       49\n",
      "dtype: int64\n",
      "0    28432\n",
      "1       49\n",
      "dtype: int64\n",
      "0    28432\n",
      "1       49\n",
      "dtype: int64\n",
      "0    28432\n",
      "1       49\n",
      "dtype: int64\n",
      "0    28431\n",
      "1       50\n",
      "dtype: int64\n",
      "0    28431\n",
      "1       50\n",
      "dtype: int64\n",
      "0    28431\n",
      "1       49\n",
      "dtype: int64\n",
      "0    28431\n",
      "1       49\n",
      "dtype: int64\n",
      "0    28431\n",
      "1       49\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Look at the test data and verify that the target training is equally distributed as possible\n",
    "for key in d_test_s_y.keys():\n",
    "    print(d_test_s_y[key].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Test the Performance of AdaBoost\n",
    "\n",
    "When we talked about AdaBoost in class, we used a collection of \"Decision Stumps\". In this assignment, we will use the implementation of [AdaBoost in Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html).  As you browse the documentation, you will notice that the default base esimator in this implentation is a `DecisionTreeClassifier(max_depth=1)` (our friend the decision stump). \n",
    "\n",
    "After you fit an AdaBoost model, you can call the method `predict` to get a class prediction, or you can call `predict_proba` to get the probability of being in the class `0` or the class `1`. These probabilities are used when creating ROC curves. \n",
    "\n",
    "Loop over the $k$ folds using the dictionaries from the first problem, and for each fold calculate the accuracy, TPR, the PPV, and the FPR.  Plot the ROC curve for each fold. You may use the [plot roc curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html) from Scikit-learn. \n",
    "\n",
    "When creating your AdaBoost classifier, please use the following parameters: \n",
    "`AdaBoostClassifier(n_estimators=25, random_state=23)`\n",
    "\n",
    "Save the predictions from the 10th fold into a variable called `y_hat_ab` for use in a future problem.\n",
    "\n",
    "Note: this took my machine a few minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def get_tp_fp_tn_fn(series_a,  series_b):\n",
    "    tp = (series_a == 1 & (series_a == series_b)).sum()\n",
    "    fp = (series_a < series_b).sum()\n",
    "    tn = (series_a == 0 & (series_a == series_b)).sum()\n",
    "    fn = (series_a > series_b).sum()\n",
    "    return (tp,fp,tn,fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "acc_ab = np.zeros(k)\n",
    "tpr_ab = np.zeros(k)\n",
    "ppv_ab = np.zeros(k)\n",
    "fpr_ab = np.zeros(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# USE THIS FOR LATER\n",
    "y_hat_ab = None\n",
    "\n",
    "# create adaboost classifer\n",
    "abc = AdaBoostClassifier(n_estimators=25, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "38 7 28432 18\n",
      "Fold: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fe16d61444af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# get current model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcurrent_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# get prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miboost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# Boosting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \"\"\"\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'SAMME.R'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \"\"\"\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iteration through training set\n",
    "fig, axis = plt.subplots()\n",
    "# there will be 10 folds\n",
    "index_fold = 0\n",
    "# d_train_df_X\n",
    "# d_train_s_y\n",
    "for key in d_train_df_X:\n",
    "    \n",
    "    # assign pointers\n",
    "    print(\"Fold: \" + str(index_fold))\n",
    "    train_x, train_y = d_train_df_X[key], d_train_s_y[key]\n",
    "    test_x, test_y = d_test_df_X[key], d_test_s_y[key]\n",
    "    \n",
    "    # get current model\n",
    "    current_model = abc.fit(train_x, train_y)\n",
    "    \n",
    "    # get prediction\n",
    "    pred_y = current_model.predict(test_x)\n",
    "    \n",
    "    if(key == 10):\n",
    "        print(\"saving last fold\")\n",
    "        y_hat_ab = pred_y\n",
    "    \n",
    "    # get total tp fp tn fn\n",
    "    total = len(test_y)\n",
    "    tp, fp, tn, fn = get_tp_fp_tn_fn(test_y, pred_y)\n",
    "    print(tp, fp, tn, fn)\n",
    "    \n",
    "    # get metrics\n",
    "    acc_ab[index_fold] = (tp + tn) / total\n",
    "    ppv_ab[index_fold] = tp / (tp + fp)\n",
    "    tpr_ab[index_fold] = tp / (tp + fn)\n",
    "    fpr_ab[index_fold] = fp / (fp + tn)\n",
    "    \n",
    "    # plot\n",
    "    nameVal = 'Index Number: ' + str(index_fold)\n",
    "    plot_roc_curve(current_model, test_x, test_y, name = nameVal, ax=axis)\n",
    "    index_fold += 1\n",
    "    \n",
    "# format\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\", shadow = True, fancybox = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The min, mean, and max TPR are: {:.2f}, {:.2f}, and {:.2f}'.format(tpr_ab.min(), tpr_ab.mean(), tpr_ab.max()))\n",
    "print('The min, mean, and max PPV are: {:.2f}, {:.2f}, and {:.2f}'.format(ppv_ab.min(), ppv_ab.mean(), ppv_ab.max()))\n",
    "print('The min, mean, and max ACC are: {:.2f}, {:.2f}, and {:.2f}'.format(acc_ab.min(), acc_ab.mean(), acc_ab.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Test the Performance of Random Forests\n",
    "\n",
    "Now, let's try another ensemble method: Random Forests, again using the [Scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). \n",
    "\n",
    "Following our book, we will build complete trees, with no pruning.  That means every leaf in the tree will be completelely pure, and if you exam an individual Decision Tree it would be overtrained to our training set.  While building the decision trees, at every internal node, we select $p$ attributes at random, and then find the best split that minimizes impurtity.  The value, $p$, is a hyperparamter of the Random Forest and corresponds to the `max_features` parameter in the Random Forest Class. \n",
    "\n",
    "After you fit an RandomForest model, you can call the method `predict` to get a class prediction, or you can call `predict_proba` to get the probability of being in the class `0` or the class `1`. These probabilities are used when creating ROC curves. \n",
    "\n",
    "Loop over the $k$ folds using the dictionaries from the first problem, and for each fold calculate the accuracy, TPR, the PPV, and the FPR.  Plot the ROC curve for each fold. You may use the [plot roc curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html) from Scikit-learn. \n",
    "\n",
    "When creating your Random Forest classifier, please use the following parameters: \n",
    "`RandomForestClassifier(criterion=\"entropy\", max_features=\"sqrt\", random_state=23)`\n",
    "\n",
    "Save the predictions from the 10th fold into a variable called `y_hat_ab` for use in a future problem.\n",
    "\n",
    "Note: This took around 20 mins on my computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "acc_rf = np.zeros(k)\n",
    "tpr_rf = np.zeros(k)\n",
    "ppv_rf = np.zeros(k)\n",
    "fpr_rf = np.zeros(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The min, mean, and max TPR are: {:.2f}, {:.2f}, and {:.2f}'.format(tpr_rf.min(), tpr_rf.mean(), tpr_rf.max()))\n",
    "print('The min, mean, and max PPV are: {:.2f}, {:.2f}, and {:.2f}'.format(ppv_rf.min(), ppv_rf.mean(), ppv_rf.max()))\n",
    "print('The min, mean, and max ACC are: {:.2f}, {:.2f}, and {:.2f}'.format(acc_rf.min(), acc_rf.mean(), acc_rf.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 Calculate the Cost of Fraud \n",
    "\n",
    "In the above problems, we saved the predictions of the 10th fold into the variables `y_hat_ab` and `y_hat_rf` for the AdaBoost and RandomForest models respectively. \n",
    "\n",
    "Now, Mr. Bank Man wants you to tell him how much money he is going to save if he deploys either of these fraud algorithms to the real-time payment processing system.  Assume that there is not a currently deployed fraud detection algorithm.  \n",
    "\n",
    "For every fraudulent transaction that is not predicted as fraudulent the bank looses twice that much money.  So, a fradulent charge for €10 is undectected, it costs the bank €20.  Also, if a charge is predicted as fradulent, but wasn't, it costs the bank a flat fee of €3 in customer service support to communicate with the customer, and mark the possible fraud as a normal transaction. \n",
    "\n",
    "Using the 10th fold test sample, calculate how much money Mr Bank Man will save with each algorithm, and make a recommendation of which algorithm to deploy to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mr Bank man will save more money, if we deploy the <> algorithm! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
